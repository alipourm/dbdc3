== Dialogue Breakdown Detection Challenge 3 ==
== development data description ==

July 28th, 2017
Sept 16th, 2017

0. Overview

ticktock_100.zip
iris_100.zip

These zip files are development data for DBDC3 in DSTC6. The original dialogue data is from the WOCHAT TickTock and IRIS dataset (see http://workshop.colips.org/wochat/data/index.html).
Each zip file contains 100 dialogue sessions as individual JSON files.

The following Two zip files are added as additional development data.

CIC_json_data.zip
YI_json_data.zip

The original dialogue data is from the CIC (The Conversational Intelligence Challenge, http://convai.io/) and the dialogues we collected by using
Yura and Idris's chatbot (bot#1337), which is participating in CIC. YI_json_data.zip has 100 dialogues in JSON. CIC_json_data.zip has 115 dialogues
in JSON as well as the same number of CONTEXT files that served as the context of the conversation. The context comes from SQuAD dataset
(https://rajpurkar.github.io/SQuAD-explorer/).

1. File name
Each JSON file contains one dialogue session, which conforms with the naming convention: <dialogue-id>.log.json

Each CONTEXT file (for CIC data only), the naming convention is <dialogue-id>.log.context
The CONTEXT file is a plain text file.

2. File format
Each file is in JSON format with UTF-8 encoding. Each file contains a dialogue that starts with a user utterance. A user and the system converse interchangeably and both utter ten times. 

Following are the top-level fields:
* dialogue-id: dialogue ID in the original dataset
* group-id: (not used)
* speaker-id: speaker (user) ID in the original dataset
* turns: array of utterances from the user and the system with breakdown annotations

Each element of the 'turns' field contains the following fields:
* utterance: the contents of an utterance 
* turn-index: index from 0 to 19 for one of the 20 utterance sequence
* time: (not used) 
* speaker: the speaker of this utterance. "U" indicates user and "S" indicates the system.
* annotations: breakdown annotations by 30 annotators (only for system utterances)

Each element of the 'annotations' field contains the following fields:
* annotator-id: crowd-worker ID in CrowdFlower
* breakdown: one of the following three labels:
    O: Not a breakdown
    T: Possibly breakdown
    X: Breakdown
* ungrammatical-sentence: (not used)
* comment: (not used) 

NOTE: Only the 'turn-index' field is numerical. All the others are textual.


3. Speakers
Refer to http://workshop.colips.org/wochat/data/index.html

Refer also to http://convai.io/

For YI_json_data.zip, the speakers were from AMT.

4. Annotators
For IRIS and TickTock, we used crowd workers from CrowdFlower for annotation.
They are 'level-2' annotators from Australia, Canada, New Zealand, United Kingdom, and United States. 
We asked the non-native English speaking workers to refrain from joining this annotation task but this is not guaranteed.
For CIC and YI datasets, we used crowd workers from AMT.

5. Miscellaneous Notes
Due to the subjective nature of this task, we did not provide any check question to be used in CrowdFlower.
Actual IRIS dialogue sessions start with a fixed system prompt. We cut out the initial prompt.

### revisions

Sept 16th, 2017: Updated with CIC_json_data.zip and YI_json_data.zip

July 28th, 2017: Updated with iris_100.zip

July 4th, 2017: Initial version with ticktock_100.zip
